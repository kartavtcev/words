{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "//import scala.io.Source\n",
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "object Helpers { // not much required for a script file\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "\n",
        "val en : String = \"en\"\n",
        "val de : String = \"de\"\n",
        "val fr : String = \"fr\"\n",
        "val langs : List[String] = List(en, de, fr)\n",
        "\n",
        "case class Lemma(entry:String, default:String, pos:String)\n",
        "\n",
        "object Lemma {\n",
        "  def parse(line:String): Option[Lemma] = {\n",
        "    line.split(\"\\t\") match { \n",
        "      case Array(f,s,t,_*) => Some(Lemma(f, s, t))\n",
        "      case _ => None\n",
        "    }\n",
        "  }  \n",
        "}\n",
        "\n",
        "val lemmasPerLang: scala.collection.mutable.Map[String, List[Lemma]] = scala.collection.mutable.Map.empty\n",
        "val stopwordsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty\n",
        "\n",
        "for(lang <- langs) {\n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-lemmatizer.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      val list = lemmasPerLang.getOrElse(lang, List.empty)\n",
        "      Lemma.parse(line) match { \n",
        "        case Some(lemma) => lemmasPerLang.update(lang, list:+lemma)\n",
        "        case _ =>;\n",
        "      }      \n",
        "    }\n",
        "  }\n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "      stopwordsPerLang.update(lang, list:+line)  \n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "for(item <- lemmasPerLang.keys) println(item)\n",
        "//for (item <- stopwordsPerLang.keys ) println(item)\n",
        "\n",
        "def detectLang(line : String) : String = { // match words from lemmatizer, TODO: Apache OpenNLP or Apache Tika\n",
        "    line.split(\" \").flatMap(item => lemmasPerLang.filter(_._2.exists(_.entry.equalsIgnoreCase(item))).map(_._1))\n",
        "                   .groupBy(f => f)\n",
        "                   .map(g => (g._1, g._2.size))\n",
        "                   .maxBy(_._2)\n",
        "                   ._1   \n",
        "}\n",
        "\n",
        "val textfilesPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty\n",
        "\n",
        "val textfiles = Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "  .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "\n",
        "for(file <- textfiles) {\n",
        "  using(scala.io.Source.fromFile(file.toString)) { source => \n",
        "    val firstLine = source.getLines.next() // get single first line & detect language with it, TODO: better to use a few random lines in the middle of the text\n",
        "    val lang : String = detectLang(firstLine)                                                 \n",
        "    var list = textfilesPerLang.getOrElse(lang, List.empty)\n",
        "    textfilesPerLang.update(lang, list:+file.toString)                                              \n",
        "  }\n",
        "}\n",
        "\n",
        "textfilesPerLang\n",
        "\n",
        "\n"
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}