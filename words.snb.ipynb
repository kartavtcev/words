{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "\n",
        "//import org.apache.spark.mllib.feature.{HashingTF, IDF}\n",
        "//import org.apache.spark.mllib.feature.HashingTF._\n",
        "//import org.apache.spark.mllib.feature.IDF._\n",
        "\n",
        "//import org.apache.spark.mllib.linalg.Vectors\n",
        "//import org.apache.spark.mllib.linalg.Vector\n",
        "//import org.apache.spark.sql.Row\n",
        "\n",
        "//import org.apache.spark.sql.SQLContext // test\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "\n",
        "val de : String = \"de\"\n",
        "val en : String = \"en\"\n",
        "val fr : String = \"fr\"\n",
        "val langs : List[String] = List(de, en, fr)\n",
        "\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "\n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"words\")\n",
        "  .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "  .master(\"local\")\n",
        "  .getOrCreate()\n",
        "val sparkContext = spark.sparkContext\n",
        "\n",
        "/*\n",
        "Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "stopwords are commonly found in the speech,\n",
        "stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "*/\n",
        "def detectLang(line : String, dicts : scala.collection.mutable.Map[String, List[String]]) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => dicts.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "}\n",
        "\n",
        "val stopwordsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty \n",
        "// stopwords per each lang, are all stored in RAM, because used for all supported languages detection & they're relatively small, quick to read.  \n",
        "\n",
        "for(lang <- langs) { // main loop. TODO: refactor into function and call it async\n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "      stopwordsPerLang.update(lang, list:+line)  \n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val textfilesPathsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty\n",
        "val textfiles = Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "                     .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "\n",
        "for(filePath <- textfiles) {\n",
        "  val file = filePath.toString\n",
        "  using(scala.io.Source.fromFile(file)) { source => \n",
        "    val firstLine = source.getLines.next() // get single first line & detect language with it, TODO: better to use a few random lines in the middle of the text\n",
        "    detectLang(firstLine, stopwordsPerLang) match  {\n",
        "      case Some(lang) => {\n",
        "        var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "        textfilesPathsPerLang.update(lang, list:+file)        \n",
        "      }\n",
        "      case None => println(\"Language was not detected for file: $file\")\n",
        "    }                                              \n",
        "  }\n",
        "}\n",
        "\n",
        "def removeTextNoise(text:String) : String = {\n",
        "  val removedNumbers = text.filter(!_.isDigit)\n",
        "  // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "  val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "  // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "  val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "  val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "  replacedEscapeSeqWithSpace\n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    val lemmas = lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "    lemmas.toSeq\n",
        "  }\n",
        "}\n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-lemmatizer.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "def removeStopWords(lang: String, tokens:Seq[String]) : Seq[String] = {\n",
        "   tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "}\n",
        "\n",
        "for ((lang,textsPaths) <- textfilesPathsPerLang) { // main loop. TODO: refactor into function and call it async\n",
        "   //val lemmas = readLemmas(lang)\n",
        "  val nlp = new OpenNLP(lang)\n",
        "  \n",
        "  for (path <- textsPaths) {\n",
        "    using(scala.io.Source.fromFile(path)) { source => \n",
        "      val text = source.getLines.mkString\n",
        "      val unnoisedText = removeTextNoise(text)                                      \n",
        "      val tokens = nlp.tokenize(unnoisedText)\n",
        "      val tokensExcludeStopWords = removeStopWords(lang, tokens)\n",
        "      val df = spark.createDataFrame(Seq((0, tokensExcludeStopWords.toArray))).toDF(\"id\", \"words\")\n",
        "                         \n",
        "      val model: CountVectorizerModel = new CountVectorizer()\n",
        "          .setInputCol(\"words\")\n",
        "          .setOutputCol(\"features\")\n",
        "          .fit(df)\n",
        "                                           \n",
        "      for(item <- model.vocabulary.take(30)){\n",
        "        print(item) \n",
        "        print(\" \")\n",
        "      }\n",
        "      \n",
        "    }\n",
        "    break; // test single file\n",
        "  }  \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "leverage money investment property times old can investments made leveraged financial owe like one way make land risky loaned lots find say market put might meltdown worth fashioned good close scala.util.control.BreakControl\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}