{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import scala.concurrent._\n",
        "import ExecutionContext.Implicits.global\n",
        "import scala.language.postfixOps\n",
        "import scala.collection.mutable.WrappedArray\n",
        "\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import org.apache.spark._\n",
        "//import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "import org.apache.spark.ml.feature.IDF\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "//object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } catch {\n",
        "            case _ : Throwable => throw new Exception(\"file exception\")\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "//}\n",
        "\n",
        "//import Helpers._\n",
        "type MapLangToStrings = scala.collection.mutable.Map[String, List[String]]\n",
        "type MapLangToStringsI = scala.collection.immutable.Map[String, List[String]]\n",
        "\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"words\")\n",
        "  .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "  .master(\"local\")\n",
        "  .getOrCreate()\n",
        "import spark.implicits._\n",
        "//val sparkContext = spark.sparkContext\n",
        "\n",
        "object NLP {  \n",
        "  def getLangs : Seq[String] = {\n",
        "    //val de : String = \"de\"\n",
        "    val en : String = \"en\"\n",
        "    //val fr : String = \"fr\"\n",
        "    //Seq(de, en, fr)\n",
        "    Seq(en)\n",
        "  }\n",
        "  \n",
        "  def getStopwordsPerLang(langs : Seq[String]) : MapLangToStringsI = {\n",
        "    langs map { lang => \n",
        "      (lang, using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) \n",
        "        { source => source.getLines.toList })\n",
        "    } toMap  \n",
        "  }\n",
        "  \n",
        "  def getFilesPaths : Seq[String] = {\n",
        "      Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "           .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "           .map(_.toString)\n",
        "           .toSeq\n",
        "  }\n",
        "}\n",
        "\n",
        "case class TupleMe(val id:(String,String)) extends Serializable\n",
        "case class WArrayMe(val tokens:WrappedArray[Any]) extends Serializable\n",
        "case class VectorMe(val idfCoefs:Vector[Double]) extends Serializable\n",
        "\n",
        "case class TfIdfFile(val id:(String,String), val tokens:Array[String], val idfCoefs:Vector[Double]) extends Serializable\n",
        "\n",
        "class NLP(val stopwordsPerLang: MapLangToStringsI, val textfilesPaths: Seq[String]) extends Serializable {\n",
        "  def this() = this(NLP.getStopwordsPerLang(NLP.getLangs), NLP.getFilesPaths)\n",
        "\n",
        "  def process(spark: SparkSession) = {\n",
        "    getFilePathsPerLang(textfilesPaths) map { case (lang, textPaths) => //Future {\n",
        "        val onlp = new OpenNLP(lang)\n",
        "  \n",
        "        val ls : Seq[((String,String), Array[String])] = textPaths map { path =>\n",
        "          using(scala.io.Source.fromFile(path)) { source =>\n",
        "            val text = source.getLines.mkString\n",
        "            val unnoisedText = removeTextNoise(text)\n",
        "                                               \n",
        "            val tokens = onlp.tokenize(unnoisedText)\n",
        "            val tokensExcludeStopWords = removeStopWords(lang, tokens, stopwordsPerLang)\n",
        "\n",
        "            val lemmas = onlp.lemmatize(tokensExcludeStopWords)\n",
        "            val lemmd = (tokensExcludeStopWords zip lemmas) map (tuple => if(tuple._2 != \"O\") tuple._2 else tuple._1 ) // if no lemma => original\n",
        "            ((lang,path),lemmd.toArray)\n",
        "          }}\n",
        "        val df = spark.createDataFrame(ls).toDF(\"id\", \"words\") \n",
        "        \n",
        "        val tf_ = new CountVectorizer()\n",
        "              .setInputCol(\"words\")\n",
        "              .setOutputCol(\"rawFeatures\") \n",
        "              .fit(df)\n",
        "        val tf = tf_.transform(df)        \n",
        "\n",
        "        val tfidf = new IDF()\n",
        "              .setInputCol(\"rawFeatures\")\n",
        "              .setOutputCol(\"features\")\n",
        "              .fit(tf)\n",
        "              .transform(tf)\n",
        "        //tfidf.printSchema\n",
        "        //val vocabulary = tf_.vocabulary\n",
        "                                                         \n",
        "        //for(v <- tfidf) println(v(0) + \"\\n\" + v(1) + \"\\n\" + v(3) + \"\\n\\n\")\n",
        "        //for(v <- tfidf) { println(v(3)(0)); println(v(3)(1)); println(v(3)(2)); }\n",
        "       \n",
        "        (tf_.vocabulary, \n",
        "          tfidf.select(\"words\", \"features\").collect.map { \n",
        "            case Row(tokens:WrappedArray[String], idfCoefs:Vector[Double]) => (tokens,idfCoefs) })                                      \n",
        "                                             \n",
        "       /*                                      \n",
        "       (tf_.vocabulary, \n",
        "        tfidf.select(\"id\", \"words\", \"features\").collect.map { \n",
        "          case Row(TupleMe(id), WArrayMe(tokens), VectorMe(idfCoefs)) => TfIdfFile(id, tokens.array, idfCoefs) })\n",
        "                   */                          \n",
        "                                             /*TfIdfFile(t(0).asInstanceOf[(String, String)], \n",
        "                                                  t(1).asInstanceOf[Array[String]], \n",
        "                                                  t(3).asInstanceOf[Vector[Double]])*/\n",
        "        // 0 - (lang,filePath)\n",
        "        // 1 - words - output to text\n",
        "        // 3 - vocab ids + tfidf coefs\n",
        "//case class TfIdfFile(val id:(String,String), val tokens:Array[String], val idfCoefs:Vector[Double]) extends Serializable\n",
        "\n",
        "    } foreach { case (_,idfCoefs) =>       for(i <- idfCoefs) print(i + \" \")\n",
        " /*case (voc, tifs) => \n",
        "      for(v <- voc) print(v + \" \")\n",
        "      for(tif <- tifs) {\n",
        "        println(tif.id)         \n",
        "        for(t <- tif.tokens) print(t + \" \")\n",
        "        println(\" \")\n",
        "        for(c <- tif.idfCoefs) print(c + \" \")\n",
        "      }*/\n",
        "    }\n",
        "    \n",
        "    //}          //break; // test single file\n",
        "  }\n",
        "  \n",
        "  def removeTextNoise(text:String) : String = {\n",
        "    val removedNumbers = text.filter(!_.isDigit)\n",
        "    // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "    val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "    // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "    val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "    val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "    replacedEscapeSeqWithSpace\n",
        "  }\n",
        "\n",
        "  def removeStopWords(lang: String, tokens:Seq[String], stopwordsPerLang : MapLangToStringsI) : Seq[String] = {\n",
        "     tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "  }\n",
        "  \n",
        "  def getFilePathsPerLang(textfilePaths : Seq[String]) : MapLangToStrings = { // TODO: to MapLangToStringsI\n",
        "    val textfilesPathsPerLang: MapLangToStrings = scala.collection.mutable.Map.empty\n",
        "  \n",
        "    for(file <- textfilePaths) {\n",
        "      using(scala.io.Source.fromFile(file)) { source => \n",
        "        val firstLine = source.getLines.next() // detect language with first line, TODO: use a few random lines in the middle of the text\n",
        "        detectLang(firstLine, stopwordsPerLang) match  {\n",
        "          case Some(lang) => {\n",
        "            var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "            textfilesPathsPerLang.update(lang, list:+file)        \n",
        "          }\n",
        "          case None => println(\"Language was not detected for file: $file\") // side effect\n",
        "        }                                              \n",
        "      }\n",
        "    }\n",
        "    textfilesPathsPerLang\n",
        "  } \n",
        "  \n",
        "  /*\n",
        "    Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "    Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "    stopwords are commonly found in the speech,\n",
        "    stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "  */\n",
        "  def detectLang(line : String, stopwordsPerLang : MapLangToStringsI) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => stopwordsPerLang.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "  } \n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) extends Serializable {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "  }\n",
        "}\n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-lemmatizer-columns-reordered.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val nlp = new NLP()\n",
        "nlp.process(spark)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:153: warning: non-variable type argument String in type pattern scala.collection.mutable.WrappedArray[String] is unchecked since it is eliminated by erasure\n            case Row(tokens:WrappedArray[String], idfCoefs:Vector[Double]) => (tokens,idfCoefs) })\n                            ^\n<console>:153: warning: non-variable type argument Double in type pattern scala.collection.immutable.Vector[Double] (the underlying of Vector[Double]) is unchecked since it is eliminated by erasure\n            case Row(tokens:WrappedArray[String], idfCoefs:Vector[Double]) => (tokens,idfCoefs) })\n                                                           ^\nLanguage was not detected for file: $file\nLanguage was not detected for file: $file\nscala.MatchError: [WrappedArray(einer, schien, für, sich, eine, völlig, logische, erklärung, gefunden, haben, warum, bei, den, grammy, award, sonntagabend, adele, für, mit, dem, hauptpreis, für, das, album, des, jahres, ausgezeichnet, wurde, und, nicht, beyoncé, für, ihr, ambitioniertes, projekt, lemonade, ich, glaube, dass, adele, gewonnen, hat, weil, sie, singen, singen, kann, sagte, altmeister, carlos, santana, zur, australischen, nachrichtenagentur, aap, und, beyoncé, bei, allem, respekt, vor, unserer, schwester, beyoncé, ist, sehr, schön, anzusehen, und, macht, eher, eine, art, model, musik, der, man, kleider, vorführen, kann, aber, sie, ist, keine, sängerin, sängerin, der, jährige, auf, dem, neuesten, stand, der, künstlerischen, entwicklung, von, beyoncé, ist, immerhin, wurde, ihr, album, lemonade, bei, erscheinen, als, dringlicher, pop, ausdruck, der, afro, amerikanischen, gegenwart, gefeiert, von, model, musik, ist, beyoncé, jedenfalls, weit, entfernt, lemonade, wurde, mit, viel, kritikerlob, überschüttet, dass, die, verblüffung, groß, war, als, sie, nur, mit, preisen, genre, unterkategorien, bei, den, grammys, ausgezeichnet, wurde, selbst, die, preisträgerin, adele, war, erstaunt, ihrer, dankesrede, sagte, die, engländerin, das, lemonade, album, sei, für, sie, derart, monumental, sie, liebe, beyoncé, auch, dafür, wie, sie, meine, schwarzen, freunde, fühlen, lässt, ermutigend, hinter, den, kulissen, wurde, adele, noch, deutlicher, ich, fand, dass, die, zeit, gekommen, ist, für, ihren, sieg, zum, teufel, muss, sie, denn, noch, tun, album, des, jahres, gewinnen, zitierte, das, branchenblatt, billboard, die, londonerin, mancher, beobachter, hatte, einen, anderen, verdacht, waren, nicht, den, vergangenen, fünf, jahren, immer, wieder, herausragende, alben, von, schwarzen, künstlern, wie, frank, ocean, kendrick, lamar, oder, eben, beyoncé, der, königskategorie, album, des, jahres, gescheitert, ein, australischer, twitter, user, hat, zusammengefasst, und, der, kommentator, des, internetmagazins, slate, brachte, den, vorwurf, auf, eine, knallige, überschrift, beyoncé, fällt, dem, rassismus, der, grammy, award, zum, opfer, beyoncés, jüngere, schwester, solange, verlinkte, ohne, größeren, kommentar, auf, ein, statement, von, frank, ocean, dem, dieser, entscheidungen, der, grammy, jury, anzweifelte, der, letzte, schwarze, musiker, der, den, hauptpreis, gewann, war, herbie, hancock, für, eine, album, hommage, joni, mitchell, einem, interview, mit, dem, musikmagazin, pitchfork, hat, nun, neil, portnow, der, präsident, der, record, academy, die, die, grammy, preise, vergibt, den, mehr, oder, weniger, expliziten, rassismus, vorwürfen, stellung, genommen, nein, ich, glaube, ganz, und, gar, nicht, dass, ein, rassimusproblem, bei, den, grammys, gibt, sagte, portnow, schließlich, spreche, man, wenn, man, von, den, grammys, rede, von, einer, akademie, mit, mitgliedern, deren, wahl, sei, demokratisch, und, zweifel, entscheide, auch, eine, mehrheit, von, nur, einer, stimme, darüber, wer, den, preis, gewinnt, und, wer, nicht, wir, als, musiker, hören, musik, nicht, auf, der, basis, von, geschlecht, oder, ethnie, glaubt, der, langjährige, plattenfirmen, vorstand, portnow, wenn, ich, über, ein, musikstück, abstimme, ist, fast, als, ich, mir, die, augen, verbinden, würde, und, einfach, zuhöre, bei, den, oscars, war, kritik, der, zusammensetzung, der, akademie, aufgekommen, die, lässt, portnow, für, das, grammy, gremium, nicht, gelten, wir, haben, dieses, problem, nicht, dem, ausmaß, aber, wir, arbeiten, natürlich, immer, daran, die, diversität, unserer, mitgliedschaft, erhöhen, punkto, ethnizität, geschlecht, genre, und, alter, carlos, santana, hat, übrigens, inzwischen, seinen, kommentar, beyoncé, klargestellt, sei, leider, aus, dem, zusammenhang, gerissen, worden, schreibt, santana, auf, seiner, facebook, seite, sei, ihm, darum, gegangen, adele, ihrer, großen, grammy, nacht, gratulieren, beyoncé, verdiene, jede, auszeichnung, die, ihr, entgegengebracht, werde, ich, wünsche, beyoncé, und, ihrer, familie, nur, das, beste, man, das, auch, über, den, sänger, cee, green, sagen, kann, der, hit, sänger, fuck, crazy, mit, gnarl, barkley, hat, einen, alten, rick, springfield, song, aus, den, achtzigerjahren, aufgegriffen, und, aus, dem, titel, jessie, girl, nun, jay, girl, gemacht, das, begehre, nun, schließen, wir, lieber, mit, den, wort, des, kinderbuchautoren, myles, johnson, der, einem, gastkommentar, für, die, new, york, time, schrieb, beyoncé, habe, etwas, gewonnen, mehr, wert, sei, als, ein, grammy, weil, sie, sich, mit, lemonade, auf, den, ausdruck, ihrer, persönlichen, erfahrung, und, ihrer, einfallskraft, konzentriert, habe, ohne, auf, den, applaus, des, männlichen, oder, weißen, publikums, schauen, habe, sie, das, leben, zahlloser, schwarzer, zuschauer, der, show, geändert, mit, ihrer, kunst, die, welt, verändert, haben, sei, ein, triumph, den, ein, künstler, für, immer, genießen, könne),(2386,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,23,24,26,28,30,31,32,33,38,41,43,44,49,51,52,61,62,64,66,69,71,76,84,85,97,103,104,110,111,115,119,120,127,130,134,135,146,154,158,161,166,173,179,182,189,197,200,202,204,215,228,236,238,241,245,256,258,266,271,274,287,299,308,319,328,331,338,340,346,351,381,390,406,407,430,431,433,445,447,463,469,476,479,493,495,496,498,508,516,523,526,527,535,546,550,552,556,557,564,576,583,585,596,601,610,612,615,619,624,634,640,647,648,657,658,671,677,680,687,695,697,704,706,710,715,717,720,726,728,732,733,737,740,748,762,769,776,779,781,785,797,800,809,811,819,841,851,865,868,874,885,894,904,907,921,923,924,931,939,944,952,959,962,968,984,993,994,1001,1007,1014,1016,1027,1045,1051,1052,1062,1064,1068,1074,1083,1084,1093,1103,1106,1109,1120,1121,1130,1144,1164,1171,1190,1201,1207,1216,1236,1245,1254,1255,1260,1268,1274,1286,1291,1294,1297,1300,1315,1330,1336,1348,1357,1364,1369,1376,1382,1384,1389,1396,1402,1417,1427,1431,1434,1444,1454,1455,1485,1506,1507,1508,1514,1521,1528,1532,1533,1537,1551,1564,1565,1567,1582,1605,1612,1619,1627,1630,1634,1636,1639,1661,1663,1664,1688,1706,1708,1715,1718,1727,1730,1731,1754,1767,1780,1782,1786,1802,1805,1808,1809,1814,1817,1844,1849,1854,1868,1887,1898,1903,1910,1945,1946,1947,1948,1958,1970,1972,1974,1977,1981,1996,1999,2000,2004,2014,2022,2023,2032,2035,2047,2050,2057,2062,2064,2071,2076,2090,2103,2129,2131,2133,2136,2141,2158,2193,2217,2220,2226,2236,2240,2242,2244,2249,2250,2252,2266,2270,2283,2313,2325,2333,2337,2341,2358,2361,2362,2369,2377,2379,2380],[11.167946633140263,5.271046405406137,7.053439978825429,4.114506654314834,9.992373303336024,3.5267199894127144,4.7022933192169525,5.290079984119071,1.7633599947063572,5.877866649021191,2.4327906486489863,2.9389333245105953,4.7022933192169525,4.114506654314834,2.3511466596084762,0.5877866649021191,5.290079984119071,2.9389333245105953,1.1755733298042381,2.3511466596084762,2.3511466596084762,4.114506654314834,3.5267199894127144,1.6218604324326575,2.9389333245105953,1.1755733298042381,1.6218604324326575,1.1755733298042381,4.7022933192169525,1.7633599947063572,1.1755733298042381,1.7633599947063572,3.5267199894127144,3.5267199894127144,18.04892876131529,1.7633599947063572,2.4327906486489863,2.9389333245105953,3.243720864865315,3.243720864865315,1.7633599947063572,0.8109302162163288,2.4327906486489863,1.1755733298042381,0.5877866649021191,1.6218604324326575,6.591673732008658,0.5877866649021191,1.6218604324326575,10.528541777433919,0.5877866649021191,1.6218604324326575,1.0986122886681098,1.6218604324326575,0.5877866649021191,9.024464380657644,0.8109302162163288,7.520386983881371,0.5877866649021191,0.8109302162163288,0.8109302162163288,0.8109302162163288,3.295836866004329,3.295836866004329,0.8109302162163288,0.5877866649021191,0.8109302162163288,0.8109302162163288,7.520386983881371,3.295836866004329,1.0986122886681098,0.8109302162163288,1.6218604324326575,1.0986122886681098,3.295836866004329,0.8109302162163288,1.0986122886681098,0.5877866649021191,2.1972245773362196,1.0986122886681098,0.8109302162163288,6.016309587105097,0.8109302162163288,0.8109302162163288,4.512232190328822,4.512232190328822,1.0986122886681098,0.8109302162163288,1.0986122886681098,2.1972245773362196,2.1972245773362196,0.8109302162163288,0.8109302162163288,4.512232190328822,4.512232190328822,0.8109302162163288,0.8109302162163288,0.8109302162163288,2.1972245773362196,1.0986122886681098,3.0081547935525483,3.0081547935525483,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098,3.0081547935525483,1.0986122886681098,3.0081547935525483,3.0081547935525483,1.0986122886681098,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,3.0081547935525483,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,3.0081547935525483,3.0081547935525483,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,3.0081547935525483,3.0081547935525483,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.0986122886681098,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n  at NLP$$anonfun$process$1$$anonfun$apply$3.apply(<console>:152)\n  at NLP$$anonfun$process$1$$anonfun$apply$3.apply(<console>:152)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n  at NLP$$anonfun$process$1.apply(<console>:152)\n  at NLP$$anonfun$process$1.apply(<console>:117)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n  at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at NLP.process(<console>:117)\n  ... 37 elided\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}