{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } catch {\n",
        "            case _ : Throwable => throw new Exception(\"file exception\")\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "type MapLangToStrings = scala.collection.mutable.Map[String, List[String]]\n",
        "\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"words\")\n",
        "  .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "  .master(\"local\")\n",
        "  .getOrCreate()\n",
        "val sparkContext = spark.sparkContext\n",
        "\n",
        "object NLP {  \n",
        "  def getLangs : Seq[String] = {\n",
        "    val de : String = \"de\"\n",
        "    val en : String = \"en\"\n",
        "    val fr : String = \"fr\"\n",
        "    Seq(de, en, fr)\n",
        "  }\n",
        "  \n",
        "  def getStopwordsPerLang(langs : Seq[String]) : MapLangToStrings = {\n",
        "    val stopwordsPerLang: MapLangToStrings = scala.collection.mutable.Map.empty \n",
        "    // stopwords per each lang, are all stored in RAM, because used for all supported languages detection & they're relatively small, quick to read.      \n",
        "\n",
        "    for(lang <- langs) {\n",
        "      using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "        for (line <- source.getLines) {\n",
        "          val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "          stopwordsPerLang.update(lang, list:+line)  \n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    stopwordsPerLang\n",
        "  }\n",
        "  \n",
        "  def getFilesPaths : Seq[String] = {\n",
        "      Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "           .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "           .map(_.toString)\n",
        "           .toSeq\n",
        "  }\n",
        "}\n",
        "\n",
        "class NLP(val stopwordsPerLang: MapLangToStrings, val textfilesPaths: Seq[String]) {\n",
        "  def this() = this(NLP.getStopwordsPerLang(NLP.getLangs), NLP.getFilesPaths)\n",
        "\n",
        "  def process = {  // main loop. TODO: refactor into function and call it async\n",
        "    for ((lang,textsPaths) <- getFilePathsPerLang(textfilesPaths)) {\n",
        "       //val lemmas = readLemmas(lang)\n",
        "      val onlp = new OpenNLP(lang)\n",
        "  \n",
        "      for (path <- textsPaths) {\n",
        "        using(scala.io.Source.fromFile(path)) { source => \n",
        "          val text = source.getLines.mkString\n",
        "          val unnoisedText = removeTextNoise(text)\n",
        "                                               \n",
        "          val tokens = onlp.tokenize(unnoisedText)\n",
        "          val tokensExcludeStopWords = removeStopWords(lang, tokens, stopwordsPerLang)\n",
        "\n",
        "          val lemmas = onlp.lemmatize(tokensExcludeStopWords)\n",
        "          val lemmd = (tokensExcludeStopWords zip lemmas).map(tuple => if(tuple._2 != \"O\") tuple._2 else tuple._1 ) // if no lemma => original\n",
        "          \n",
        "          val df = spark.createDataFrame(Seq((0, lemmd.toArray))).toDF(\"id\", \"words\")                         \n",
        "          val model: CountVectorizerModel = new CountVectorizer()\n",
        "            .setInputCol(\"words\")\n",
        "            .setOutputCol(\"features\")\n",
        "            .fit(df)\n",
        "                                               \n",
        "          for(item <- model.vocabulary) println(item)\n",
        "        }\n",
        "        break; // test single file\n",
        "      }  \n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def removeTextNoise(text:String) : String = {\n",
        "    val removedNumbers = text.filter(!_.isDigit)\n",
        "    // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "    val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "    // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "    val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "    val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "    replacedEscapeSeqWithSpace\n",
        "  }\n",
        "\n",
        "  def removeStopWords(lang: String, tokens:Seq[String], stopwordsPerLang : MapLangToStrings) : Seq[String] = {\n",
        "     tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "  }\n",
        "  \n",
        "  def getFilePathsPerLang(textfilePaths : Seq[String]) : MapLangToStrings = {\n",
        "    val textfilesPathsPerLang: MapLangToStrings = scala.collection.mutable.Map.empty\n",
        "  \n",
        "    for(file <- textfilePaths) {\n",
        "      using(scala.io.Source.fromFile(file)) { source => \n",
        "        val firstLine = source.getLines.next() // get single first line & detect language with it, TODO: better to use a few random lines in the middle of the text\n",
        "        detectLang(firstLine, stopwordsPerLang) match  {\n",
        "          case Some(lang) => {\n",
        "            var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "            textfilesPathsPerLang.update(lang, list:+file)        \n",
        "          }\n",
        "          case None => println(\"Language was not detected for file: $file\")\n",
        "        }                                              \n",
        "      }\n",
        "    }\n",
        "    textfilesPathsPerLang\n",
        "  } \n",
        "  \n",
        "  /*\n",
        "    Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "    Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "    stopwords are commonly found in the speech,\n",
        "    stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "  */\n",
        "  def detectLang(line : String, stopwordsPerLang : MapLangToStrings) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => stopwordsPerLang.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "  } \n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "  }\n",
        "}\n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-lemmatizer-columns-reordered.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val nlp = new NLP()\n",
        "nlp.process\n",
        "/*\n",
        "import scala.io.Source\n",
        "import java.io._\n",
        "for (lang <- NLP.getLangs) {\n",
        "      val writer = new PrintWriter(new File(s\"notebooks/words/vocabs/$lang-lemmatizer-columns-reordered.txt\"))\n",
        "      using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-lemmatizer.txt\")) { source => \n",
        "        for (line <- source.getLines) {\n",
        "           val array = line.split(\"\\t\")\n",
        "           writer.write(s\"${array(0)}\\t${array(2)}\\t${array(1)}\\n\")\n",
        "        }\n",
        "      }\n",
        "      writer.close()\n",
        "}\n",
        "*/"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "investment\nmake\nleverage\nmoney\ntime\nproperty\nleveraged\nloan\ncan\ngood\nold\nowe\nborrow\nvalue\none\nlot\nfinancial\nway\nlike\nlose\nprofit\nsay\nmarket\nput\nworth\nunderstand\nbuy\nmight\ntear\nmeltdown\nrisky\nland\ninstitution\nratio\nfind\ndownturn\ninvest\ninterest\nget\ncall\nhair\nalso\njust\nfashion\nseem\nwant\nstart\nincrease\nclose\ncrisis\nanother\nburst\nrepeat\nlead\nnormal\neconomy\namount\nlittle\nbubble\nbad\nlet\nperson\neven\nday\ncome\nfactor\nneed\nnew\nnow\nsee\ndrop\ntechnique\nsystem\nback\nstill\nsensible\nshit\ninstead\nleave\nperiod\nkind\ncertainly\nselling\npolicy\nsupposedly\nlook\nrelatively\nattractive\nhilt\nwave\nlink\nlesson\nsurf\ndoubting\nloss\nmortgage\nhowever\nedge\narticle\nnasty\ninternet\nguy\ncredit\nclosely\nwrong\nrun\nunreasonable\nokay\nsenseless\nworld\nstrict\nstop\nbankrupt\nparticular\nstuff\nhappen\ntick\nmac\nsum\nonto\njohn\ncompany\nnicely\nable\nrefer\npractice\nmany\nputt\nvoice\nlend\ninsist\nbecome\nwouldn\npay\nlong\nsmall\nlevel\ntidal\ncrash\ntotal\nuse\ngovernment\npowerful\npoint\nunwittingly\nreach\nsingle\noriginal\nfashioned\ninstance\nhaven\nill\nstandard\neasy\ndifficult\nhappy\nthing\ngiant\nterrible\nwow\ndangerous\nnice\nmean\ncut\narguably\nregulation\nsupport\nbrink\nunderstanding\ncourse\narcane\nkey\nattraction\nwork\nactually\nfuture\nprevent\nstepek\nyork\nfannie\neconomic\nmid\ndefault\nrecently\ncomplete\nweek\nisn\nmay\nbring\nshed\nquite\nagent\navoid\nbig\nfirst\nsend\nextremely\nproblem\ncompare\nexample\ndeal\nlimit\nhalf\npersuade\nwasn\nban\ncount\nbank\nhigh\nbrake\nblame\nwell\nago\nsell\nlearnt\nbusiness\nwait\nwill\nswap\nimportant\nactivity\ncliff\nhold\nessence\nfreddie\nfinance\nscala.util.control.BreakControl\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}