{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "\n",
        "//import org.apache.spark.mllib.feature.{HashingTF, IDF}\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "\n",
        "// import org.apache.spark.ml.feature.StopWordsRemover // TODO:\n",
        "\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "\n",
        "//import org.apache.spark.mllib.feature.HashingTF._\n",
        "//import org.apache.spark.mllib.feature.IDF._\n",
        "\n",
        "//import org.apache.spark.mllib.linalg.Vectors\n",
        "//import org.apache.spark.mllib.linalg.Vector\n",
        "//import org.apache.spark.sql.Row\n",
        "\n",
        "//import org.apache.spark.sql.SQLContext // test\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import opennlp.tools.langdetect._\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "//import opennlp.tools.util.Span\n",
        "\n",
        "object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "\n",
        "val de : String = \"de\"\n",
        "val en : String = \"en\"\n",
        "val fr : String = \"fr\"\n",
        "val langs : List[String] = List(de, en, fr)\n",
        "\n",
        "\n",
        "val conf = new SparkConf().setAppName(\"words\")\n",
        "conf.set(\"spark.driver.allowMultipleContexts\", \"true\");\n",
        "conf.setMaster(\"local\");\n",
        "val sparkContext = new SparkContext(conf)\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "\n",
        "val spark = SparkSession.builder().appName(\"words\").master(\"local\").getOrCreate()\n",
        "\n",
        "def detectLang(line : String, dicts : scala.collection.mutable.Map[String, List[String]]) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => dicts.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "}\n",
        "\n",
        "val stopwordsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty \n",
        "// Stopwords per each lang, are all stored in RAM, because used for all supported languages detection & they're relatively small, quick to read.  \n",
        "\n",
        "for(lang <- langs) { // main loop. TODO: refactor into function and call it async\n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "      stopwordsPerLang.update(lang, list:+line)  \n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val textfilesPathsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty\n",
        "val textfiles = Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "                     .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "\n",
        "for(filePath <- textfiles) {\n",
        "  val file = filePath.toString\n",
        "  using(scala.io.Source.fromFile(file)) { source => \n",
        "    val firstLine = source.getLines.next() // get single first line & detect language with it, TODO: better to use a few random lines in the middle of the text\n",
        "    detectLang(firstLine, stopwordsPerLang) match  {\n",
        "      case Some(lang) => {\n",
        "        var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "        textfilesPathsPerLang.update(lang, list:+file)        \n",
        "      }\n",
        "      case None => println(\"Language was not detected for file: $file\")\n",
        "    }                                              \n",
        "  }\n",
        "}\n",
        "/*\n",
        "case class Lemma(entry:String, default:String, pos:String)\n",
        "object Lemma {\n",
        "  def parse(line:String): Option[Lemma] = {\n",
        "    line.split(\"\\t\") match { \n",
        "      case Array(f,s,t,_*) => Some(Lemma(f, s, t))\n",
        "      case _ => None\n",
        "    }\n",
        "  }  \n",
        "}\n",
        "*/\n",
        "// ?\n",
        "def removeTextNoise(text:String) : String = {\n",
        "  val removedNumbers = text.filter(!_.isDigit)\n",
        "  // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "  val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "  // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "  val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "  val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "  replacedEscapeSeqWithSpace\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        "def readLemmas(lang:String) : scala.collection.mutable.ListBuffer[Lemma] = {\n",
        "  var lemmas : scala.collection.mutable.ListBuffer[Lemma] = scala.collection.mutable.ListBuffer.empty  \n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-lemmatizer.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      Lemma.parse(line) match { \n",
        "        case Some(lemma) => lemmas+=lemma\n",
        "        case _ =>;\n",
        "      }      \n",
        "    }\n",
        "  }\n",
        "  lemmas\n",
        "}\n",
        "*/\n",
        "/*\n",
        "  def initTokenizer(lang:String) = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      val model : TokenizerModel = new TokenizerModel(stream)\n",
        "      new TokenizerME(model);\n",
        "    }\n",
        "*/\n",
        "class OpenNlpTokenizer(val model: TokenizerModel) {//extends Tokenizer {\n",
        "  def this(lang:String) = this(OpenNlpTokenizer.loadDefaultModel(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(model)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    //assume(positions.length == strings.length)\n",
        "    //for ((pos, string) <- (positions.iterator zip strings.iterator).toList)\n",
        "    //yield new Token(string, pos.getStart)\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }  \n",
        "}\n",
        "\n",
        "object OpenNlpTokenizer {\n",
        "  def loadDefaultModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "/*\n",
        "object OpenNlpTokenizerMain extends TokenizerMain {\n",
        "  val tokenizer = new OpenNlpTokenizer()\n",
        "}\n",
        "*/\n",
        "\n",
        "def removeStopWords(lang: String, tokens:Seq[String]) : Seq[String] = {\n",
        "   tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "}\n",
        "\n",
        "\n",
        "for ((lang,textsPaths) <- textfilesPathsPerLang) { // main loop. TODO: refactor into function and call it async\n",
        "   //val lemmas = readLemmas(lang)\n",
        "  val tokenizer = new OpenNlpTokenizer(lang)\n",
        "  \n",
        "  for (paths <- textsPaths) {\n",
        "    using(scala.io.Source.fromFile(paths)) { source => \n",
        "      val text = source.getLines.mkString // getLines.toList for RDD...\n",
        "      val unnoisedText = removeTextNoise(text)                                      \n",
        "      val tokens = tokenizer.tokenize(unnoisedText)\n",
        "      val tokensWithoutStopWords = removeStopWords(lang, tokens)\n",
        "      //val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\n",
        "      //import sqlContext.implicits._\n",
        "         // : RDD[Seq[String]]                                    \n",
        "      //val rdd = sparkContext.parallelize(tokens)\n",
        "                                            //.map{ x:Row => x.getAs[String](0)}\n",
        "                                            //.map(x => Tuple1(x.split(\",\"))) // wrapping\n",
        "      //val tf: RDD[Vector] = new HashingTF().transform(rdd)\n",
        "      //tf.cache()                                                                   \n",
        "      //val idf = new IDF().fit(tf)\n",
        "      //val tfidf: RDD[Vector] = idf.transform(tf)\n",
        "      //tfidf.foreach(x => println(x))\n",
        "      \n",
        "      //val tf = new HashingTF().transform(rdd)\n",
        "      //val idf = new IDF().fit(tf)\n",
        "      //val tfidf = idf.transform(tf)\n",
        "      for(item <- tokensWithoutStopWords){\n",
        "        print(item) \n",
        "        print(\" \")\n",
        "      }\n",
        "      //rescaledData.select(\"features\").show()\n",
        "      \n",
        "      //val spark: SparkSession\n",
        "      // Seq(1, tokens)\n",
        "                                            \n",
        "                                            \n",
        "                                            /*\n",
        "      val df = spark.createDataFrame(Array((1, tokensWithoutStopWords))).toDF(\"id\", \"tokens\") \n",
        "\n",
        "      val model: CountVectorizerModel = new CountVectorizer()\n",
        "          .setInputCol(\"tokens\")\n",
        "          .setOutputCol(\"features\")\n",
        "          .fit(df)\n",
        "      model.vocabulary\n",
        "      */                                                \n",
        "    }\n",
        "    break; // do not overflow the sp\n",
        "  }  \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "world financial system came close complete meltdown one wants repeat terrible crash can repeat avoided first instance need understand brought brink wrong good times unwittingly leading cliff edge just led meltdown certainly wasn single factor blame mid financial institutions putting money new kinds risky investments investments like credit default swaps difficult understand new york times called arcane one article particular investments less important factor technique called leverage used make investments arguably understanding leverage key understanding meltdown leverage essence just refers practice borrowing money make investment see works see attractive can seem extremely risky lets compare leverage old fashioned investment say invest property buy land worth period time market value increases make bad old days might happy nice better wouldn leverage economy ticking quite nicely able persuade financial institution lend lot let say loaned times original amount making total sum wow invest property value increases sell property count profits find made instead profit made old fashioned technique made course pay interest money borrowed might cut profits half profit still way way better times good doubting attractions leverage bubble bursts things can get nasty old days bought property worth money land values dropped hold onto property shed tears losing wait good times come back happens leveraged investment well drop land values investment now worth market looks bad people loaned money want back loaned times can get selling property still owe another shit started long ago now owe haven lost lost actually even worse also owe interest loan another lose money owe meaning made loss start tearing hair tearing hair financial institution isn old fashioned bank also leveraged hilt made lots lots loans like leveraged way relatively small downturn market guy buys stuff money downturn big deal can send tidal wave investment finance business leaving lots companies bankrupt example leverage ratio one might seem like senseless amount borrowing normal people however bubble burst ratios even higher mortgage giants fannie may freddie mac closely linked government supposedly run stricter standards normal leveraged close surf little internet investment agents find say okay leveraged one borrow put risky investments lesson learnt john stepek put money week recently way stop future crises prevent level leverage system reaching point becomes dangerous leverage ratios sensible limit need banned problem easy find support policy like crisis economy many powerful voices will insist regulations put unreasonable brake economic activity scala.util.control.BreakControl\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}