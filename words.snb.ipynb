{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4",
      "com.fasterxml.jackson.core % jackson-core % 2.9.3"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.collection.JavaConversions._\n",
        "import scala.concurrent._\n",
        "import ExecutionContext.Implicits.global\n",
        "import scala.language.postfixOps\n",
        "import scala.collection.mutable.WrappedArray\n",
        "\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.sql.{Row, SparkSession, Dataset}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
        "import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n",
        "import com.fasterxml.jackson.databind.{ObjectMapper, DeserializationFeature}\n",
        "import com.fasterxml.jackson.annotation.JsonProperty\n",
        "\n",
        "// using .close() is not strictly required for sripts\n",
        "def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "  try {\n",
        "      f(resource)\n",
        "  } catch {\n",
        "      case _ : Throwable => throw new Exception(\"file exception\")\n",
        "  } finally {\n",
        "      resource.close()\n",
        "  }\n",
        "\n",
        "type MapLangToStrings = scala.collection.immutable.Map[String, Seq[String]]\n",
        "   /*\n",
        "   since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "   could be related to customDepts bug, because I added opennlp dependency:\n",
        "   https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "   */\n",
        "  lazy val spark = SparkSession\n",
        "    .builder()\n",
        "    .appName(\"words\")\n",
        "    .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "    .master(\"local\")\n",
        "    .getOrCreate()\n",
        "import spark.implicits._\n",
        "\n",
        "object NLP { \n",
        "  \n",
        "  case class Keyword(\n",
        "    @JsonProperty(\"keyword\")@JsonProperty(required = true) keyword: String,\n",
        "    @JsonProperty(\"value\")@JsonProperty(required = true) value: Double\n",
        "  )\n",
        "  case class TfIdfFile(\n",
        "    @JsonProperty(\"filename\")@JsonProperty(required = true) filename: String,\n",
        "    @JsonProperty(\"language\")@JsonProperty(required = true) language:String,\n",
        "    @JsonProperty(\"keywords\")@JsonProperty(required = true) features : Seq[Keyword]\n",
        "  )\n",
        "  case class ProcessedFile(\n",
        "    filename: String,\n",
        "    words: Seq[String]\n",
        "  )\n",
        "  \n",
        "  val numOfTopWords = 30  \n",
        "  \n",
        "  val vocabsRelativePath = \"notebooks/words/vocabs/\"\n",
        "  val textsRelativePath = \"notebooks/words/text-data/\"\n",
        "  val outputPath = \"notebooks/words/output/\"\n",
        "  val langNotDetected = \"lang-not-detected\"\n",
        "  \n",
        "  def getLangs : Seq[String] = {\n",
        "    val de : String = \"de\"\n",
        "    val en : String = \"en\"\n",
        "    val fr : String = \"fr\"\n",
        "    Seq(de, en, fr)\n",
        "  }\n",
        "  \n",
        "  def getStopwordsPerLang(langs : Seq[String]) : MapLangToStrings = {\n",
        "    langs map { lang => (lang, using(scala.io.Source.fromFile(s\"${vocabsRelativePath}${lang}-stopwords.txt\")) { \n",
        "                                 source => source.getLines.toList })\n",
        "    } toMap  \n",
        "  }\n",
        "  \n",
        "  def getFilesPaths : Seq[String] = {\n",
        "      Files.newDirectoryStream(Paths.get(textsRelativePath))\n",
        "           .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "           .map(_.toString)\n",
        "           .toSeq.sorted\n",
        "  }\n",
        "  \n",
        "  def writeProcessedFile(pf: Seq[ProcessedFile]) = {}\n",
        "  def writeTfIdfJson(tfidf: Seq[TfIdfFile]) = {\n",
        "      using (new BufferedWriter(new FileWriter(new File(s\"${outputPath}/tfidf_files.json\")))) { bw =>\n",
        "        bw.write(getJsonMapper().writeValueAsString(tfidf))\n",
        "      }    \n",
        "  }\n",
        "  \n",
        "  def getJsonMapper() : ObjectMapper = {\n",
        "    val mapper = new ObjectMapper with ScalaObjectMapper\n",
        "    mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)\n",
        "    mapper.registerModule(DefaultScalaModule)\n",
        "    mapper\n",
        "  }\n",
        "}\n",
        "\n",
        "class NLP(val stopwordsPerLang: MapLangToStrings, val textfilesPaths: Seq[String]) {\n",
        "  def this() = this(NLP.getStopwordsPerLang(NLP.getLangs), NLP.getFilesPaths)\n",
        "\n",
        "  def process(spark: SparkSession) : Seq[Either[(String, Seq[String]),(Seq[NLP.ProcessedFile], Seq[NLP.TfIdfFile])]] = {\n",
        "    getFilePathsPerLang(textfilesPaths) map { case (lang, textPaths) => //Future {\n",
        "        if(lang == NLP.langNotDetected) Left((lang, textPaths))\n",
        "                                             \n",
        "        val onlp = new OpenNLP(lang)\n",
        "  \n",
        "        val ls : Seq[(String,String,Array[String])] = textPaths map { path =>\n",
        "          using(scala.io.Source.fromFile(path)) { source =>\n",
        "            val text = source.getLines.mkString\n",
        "            val unnoisedText = removeTextNoise(text)\n",
        "                                               \n",
        "            val tokens = onlp.tokenize(unnoisedText)\n",
        "            val tokensExcludeStopWords = removeStopWords(lang, tokens, stopwordsPerLang)\n",
        "\n",
        "            val lemmas = onlp.lemmatize(tokensExcludeStopWords)\n",
        "            val lemmd = (tokensExcludeStopWords zip lemmas) map (tuple => if(tuple._2 != \"O\") tuple._2 else tuple._1 ) // if no lemma => original\n",
        "            (lang,path.split(\"/\").takeRight(1).head,lemmd.toArray)\n",
        "          }}\n",
        "        val df = spark.createDataFrame(ls).toDF(\"language\", \"filename\", \"tokens\") \n",
        "        val tf = new CountVectorizer()\n",
        "              .setInputCol(\"tokens\")\n",
        "              .setOutputCol(\"tf\")        \n",
        "\n",
        "        val idf = new IDF()\n",
        "              .setInputCol(\"tf\")\n",
        "              .setOutputCol(\"tfidf\")\n",
        "        \n",
        "        val va = new VectorAssembler()\n",
        "              .setInputCols(Array(\"tfidf\"))\n",
        "              .setOutputCol(\"features\")\n",
        "                                             \n",
        "        val tfidfPipeline = new Pipeline().setStages(Array(tf, idf, va))\n",
        "        val pipelineModel = tfidfPipeline.fit(df)\n",
        "        val tfidf =  pipelineModel.transform(df)\n",
        "        \n",
        "        val cvModel = pipelineModel.stages(0).asInstanceOf[CountVectorizerModel]      \n",
        "        val vocabulary = cvModel.vocabulary\n",
        "                                             \n",
        "        val tis = tfidf.select(\"filename\", \"language\", \"features\")\n",
        "                       .map { case Row(filename: String, language: String, features: Vector) => ((filename, language), features.toArray.toSeq)}        \n",
        "                       .collect().toSeq\n",
        "        val pfs = df.select(\"filename\", \"tokens\")                                             \n",
        "                    .map { case Row(filename: String, tokens: WrappedArray[String]) => (filename, tokens.toSeq)}\n",
        "                    .collect().toSeq\n",
        "                               \n",
        "        Right((pfs.map(pf => NLP.ProcessedFile(pf._1, pf._2))),\n",
        "               tis.map( ti =>\n",
        "                 NLP.TfIdfFile(ti._1._1, ti._1._2, ti._2.zipWithIndex\n",
        "                                                              .sortBy(- _._1)\n",
        "                                                              .take(NLP.numOfTopWords)\n",
        "                                                              .map(f => NLP.Keyword(vocabulary(f._2), f._1)))))\n",
        "           \n",
        "    } toSeq\n",
        "  }\n",
        "   \n",
        "  def removeTextNoise(text:String) : String = {\n",
        "    val removedNumbers = text.filter(!_.isDigit)\n",
        "    // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "    val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "    // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "    val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "    val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "    replacedEscapeSeqWithSpace\n",
        "  }\n",
        "\n",
        "  def removeStopWords(lang: String, tokens:Seq[String], stopwordsPerLang : MapLangToStrings) : Seq[String] = {\n",
        "     tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "  }\n",
        "  \n",
        "  def getFilePathsPerLang(textfilePaths : Seq[String]) : MapLangToStrings = {\n",
        "    textfilePaths map { file => \n",
        "      using(scala.io.Source.fromFile(file)) { source => \n",
        "        val firstLine = source.getLines.next() // detect language with first line, TODO: use a few random lines in the middle of the text\n",
        "        detectLang(firstLine, stopwordsPerLang) match  {\n",
        "          case Some(lang) => (lang, file)    \n",
        "          case None => (NLP.langNotDetected, file)\n",
        "        }                                              \n",
        "      }    \n",
        "    } groupBy(_._1) map { case (lang, group) => (lang, group.map(_._2)) }\n",
        "  } \n",
        "  \n",
        "  /*\n",
        "    Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "    Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "    stopwords are commonly found in the speech,\n",
        "    stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "  */\n",
        "  def detectLang(line : String, stopwordsPerLang : MapLangToStrings) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => stopwordsPerLang.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "  } \n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "  }\n",
        "} \n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-lemmatizer-columns-reordered.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "try { // for people who're looking for \"error handling\"\n",
        "  new NLP().process(spark) \n",
        "    .foldRight(\n",
        "        Right((Seq.empty[NLP.ProcessedFile], Seq.empty[NLP.TfIdfFile])): \n",
        "          Either[(String, Seq[String]),(Seq[NLP.ProcessedFile], Seq[NLP.TfIdfFile])]) { (elem, acc) =>\n",
        "      for {\n",
        "        t <- acc.right\n",
        "        h <- elem.right    \n",
        "      } yield (h._1++t._1, h._2++t._2)\n",
        "    }\n",
        "    match {\n",
        "      case Right((file, tfidfjson)) => \n",
        "        { NLP writeProcessedFile(file); NLP writeTfIdfJson(tfidfjson); } \n",
        "      case Left((lang, files)) => \n",
        "        files foreach (f => println(\"Lang: $lang is not detected for file: $f\"))}\n",
        "} catch {\n",
        "  case e: Exception => println(e)\n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "<console>:183: warning: non-variable type argument String in type pattern scala.collection.mutable.WrappedArray[String] is unchecked since it is eliminated by erasure\n                    .map { case Row(filename: String, tokens: WrappedArray[String]) => (filename, tokens.toSeq)}\n                                                              ^\nimport scala.collection.JavaConversions._\nimport scala.concurrent._\nimport ExecutionContext.Implicits.global\nimport scala.language.postfixOps\nimport scala.collection.mutable.WrappedArray\nimport java.io._\nimport java.nio.file.{Files, Path, Paths}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.{Row, SparkSession, Dataset}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport opennlp.tools.lemmatizer.DictionaryLemmatizer\nimport opennlp.tools.postag.{POSModel, POSTaggerME}\nimport opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport com.fasterxml.jackson.d..."
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 62,
          "time" : "Took: 37.675s, at 2018-01-24 05:08"
        }
      ]
    }
  ],
  "nbformat" : 4
}