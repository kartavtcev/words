{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import scala.concurrent._\n",
        "import ExecutionContext.Implicits.global\n",
        "import scala.language.postfixOps\n",
        "\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } catch {\n",
        "            case _ : Throwable => throw new Exception(\"file exception\")\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "type MapLangToStrings = scala.collection.mutable.Map[String, List[String]]\n",
        "type MapLangToStringsI = scala.collection.immutable.Map[String, List[String]]\n",
        "\n",
        "\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "val spark = SparkSession\n",
        "  .builder()\n",
        "  .appName(\"words\")\n",
        "  .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "  .master(\"local\")\n",
        "  .getOrCreate()\n",
        "val sparkContext = spark.sparkContext\n",
        "\n",
        "object NLP {  \n",
        "  def getLangs : Seq[String] = {\n",
        "    val de : String = \"de\"\n",
        "    val en : String = \"en\"\n",
        "    val fr : String = \"fr\"\n",
        "    Seq(de, en, fr)\n",
        "  }\n",
        "  \n",
        "  def getStopwordsPerLang(langs : Seq[String]) : MapLangToStringsI = {/*\n",
        "    val stopwordsPerLang: MapLangToStrings = scala.collection.mutable.Map.empty \n",
        "    // stopwords per each lang, are all stored in RAM, because used for all supported languages detection & they're relatively small, quick to read.      \n",
        "\n",
        "    for(lang <- langs) {\n",
        "      using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "        for (line <- source.getLines) {\n",
        "          val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "          stopwordsPerLang.update(lang, list:+line)  \n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    stopwordsPerLang\n",
        "    */\n",
        "    langs map { lang => \n",
        "      (lang, using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => source.getLines.toList })\n",
        "    } toMap  \n",
        "  }\n",
        "  \n",
        "  def getFilesPaths : Seq[String] = {\n",
        "      Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "           .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "           .map(_.toString)\n",
        "           .toSeq\n",
        "  }\n",
        "}\n",
        "\n",
        "class NLP(val stopwordsPerLang: MapLangToStringsI, val textfilesPaths: Seq[String]) {\n",
        "  def this() = this(NLP.getStopwordsPerLang(NLP.getLangs), NLP.getFilesPaths)\n",
        "\n",
        "  def process = {\n",
        "    getFilePathsPerLang(textfilesPaths) foreach { case (lang, textPaths) => //Future {\n",
        "        val onlp = new OpenNLP(lang)\n",
        "  \n",
        "        //for (path <- textPaths) {\n",
        "        val ls : Seq[(Int, Array[String])] = textPaths.map { path =>\n",
        "          using(scala.io.Source.fromFile(path)) { source =>\n",
        "            val text = source.getLines.mkString\n",
        "            val unnoisedText = removeTextNoise(text)\n",
        "                                               \n",
        "            val tokens = onlp.tokenize(unnoisedText)\n",
        "            val tokensExcludeStopWords = removeStopWords(lang, tokens, stopwordsPerLang)\n",
        "\n",
        "            val lemmas = onlp.lemmatize(tokensExcludeStopWords)\n",
        "            val lemmd = (tokensExcludeStopWords zip lemmas) map (tuple => if(tuple._2 != \"O\") tuple._2 else tuple._1 ) // if no lemma => original\n",
        "            lemmd.toArray\n",
        "          }}.zipWithIndex.map { case (array, index) => (index, array) }\n",
        "            // TODO: \n",
        "            //val df = spark.createDataFrame(Seq((0, lemmd.toArray))).toDF(\"id\", \"words\")                         \n",
        "            //val vectorizer: CountVectorizerModel = \n",
        "            val df = spark.createDataFrame(ls).toDF(\"id\", \"words\") \n",
        "            \n",
        "            val tf = new CountVectorizer()\n",
        "              .setInputCol(\"words\")\n",
        "              .setOutputCol(\"rawFeatures\")\n",
        "              .fit(df)\n",
        "              .transform(df)\n",
        "                                               \n",
        "            //val featurizedData = vectorizer.vocabulary\n",
        "            //for(item <- featurizedData) println(item)\n",
        "\n",
        "            \n",
        "            //val tf = vectorizer.transform(df)\n",
        "            //vector.\n",
        "            //for(item <- vector) println(item)\n",
        "            //print(vector.shape)\n",
        "            //print(type(vector))\n",
        "            //print(vector.toArray())\n",
        "            //print(vector)\n",
        "            \n",
        "            /*\n",
        "            val sentenceData = spark.createDataFrame(Seq(\n",
        "              (0.0, \"Hi I heard about Spark\"),\n",
        "              (0.0, \"I wish Java could use case classes\"),\n",
        "              (1.0, \"Logistic regression models are neat\")\n",
        "            )).toDF(\"label\", \"sentence\")\n",
        "*/\n",
        "            //val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
        "            //val wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "            //val hashingTF = new HashingTF()\n",
        "              //.setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
        "\n",
        "            //val featurizedData = hashingTF.transform(wordsData)\n",
        "            // alternatively, CountVectorizer can also be used to get term frequency vectors            \n",
        "            \n",
        "            //val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").fit(tf)\n",
        "\n",
        "            val tfidf = new IDF()\n",
        "              .setInputCol(\"rawFeatures\")\n",
        "              .setOutputCol(\"features\")\n",
        "              .fit(tf)\n",
        "              .transform(tf)\n",
        "            //tfidf.select(\"id\", \"features\").show()\n",
        "            for(item <- tfidf) println(item)\n",
        "          }\n",
        "          //break; // test single file        \n",
        "      //}\n",
        "      //}\n",
        "    //}\n",
        "  }\n",
        "  \n",
        "  def removeTextNoise(text:String) : String = {\n",
        "    val removedNumbers = text.filter(!_.isDigit)\n",
        "    // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "    val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "    // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "    val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "    val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "    replacedEscapeSeqWithSpace\n",
        "  }\n",
        "\n",
        "  def removeStopWords(lang: String, tokens:Seq[String], stopwordsPerLang : MapLangToStringsI) : Seq[String] = {\n",
        "     tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "  }\n",
        "  \n",
        "  def getFilePathsPerLang(textfilePaths : Seq[String]) : MapLangToStrings = { // TODO: to MapLangToStringsI\n",
        "    val textfilesPathsPerLang: MapLangToStrings = scala.collection.mutable.Map.empty\n",
        "  \n",
        "    for(file <- textfilePaths) {\n",
        "      using(scala.io.Source.fromFile(file)) { source => \n",
        "        val firstLine = source.getLines.next() // detect language with first line, TODO: use a few random lines in the middle of the text\n",
        "        detectLang(firstLine, stopwordsPerLang) match  {\n",
        "          case Some(lang) => {\n",
        "            var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "            textfilesPathsPerLang.update(lang, list:+file)        \n",
        "          }\n",
        "          case None => println(\"Language was not detected for file: $file\")\n",
        "        }                                              \n",
        "      }\n",
        "    }\n",
        "    textfilesPathsPerLang\n",
        "  } \n",
        "  \n",
        "  /*\n",
        "    Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "    Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "    stopwords are commonly found in the speech,\n",
        "    stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "  */\n",
        "  def detectLang(line : String, stopwordsPerLang : MapLangToStringsI) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => stopwordsPerLang.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "  } \n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "  }\n",
        "}\n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-lemmatizer-columns-reordered.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val nlp = new NLP()\n",
        "nlp.process"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "[0,WrappedArray(world, financial, system, come, close, complete, meltdown, one, want, repeat, terrible, crash, can, repeat, avoid, first, instance, need, understand, bring, brink, wrong, good, time, unwittingly, lead, cliff, edge, just, lead, meltdown, certainly, wasn, single, factor, blame, mid, financial, institution, putt, money, new, kind, risky, investment, investment, like, credit, default, swap, difficult, understand, new, york, time, call, arcane, one, article, particular, investment, little, important, factor, technique, call, leverage, use, make, investment, arguably, understand, leverage, key, understanding, meltdown, leverage, essence, just, refer, practice, borrow, money, make, investment, see, work, see, attractive, can, seem, extremely, risky, let, compare, leverage, old, fashioned, investment, say, invest, property, buy, land, worth, period, time, market, value, increase, make, bad, old, day, might, happy, nice, good, wouldn, leverage, economy, tick, quite, nicely, able, persuade, financial, institution, lend, lot, let, say, loan, time, original, amount, make, total, sum, wow, invest, property, value, increase, sell, property, count, profit, find, make, instead, profit, make, old, fashion, technique, make, course, pay, interest, money, borrow, might, cut, profit, half, profit, still, way, way, good, time, good, doubting, attraction, leverage, bubble, burst, thing, can, get, nasty, old, day, buy, property, worth, money, land, value, drop, hold, onto, property, shed, tear, lose, wait, good, time, come, back, happen, leveraged, investment, well, drop, land, value, investment, now, worth, market, look, bad, person, loan, money, want, back, loan, time, can, get, selling, property, still, owe, another, shit, start, long, ago, now, owe, haven, lose, lose, actually, even, ill, also, owe, interest, loan, another, lose, money, owe, mean, make, loss, start, tear, hair, tear, hair, financial, institution, isn, old, fashion, bank, also, leveraged, hilt, make, lot, lot, loan, like, leveraged, way, relatively, small, downturn, market, guy, buy, stuff, money, downturn, big, deal, can, send, tidal, wave, investment, finance, business, leave, lot, company, bankrupt, example, leverage, ratio, one, might, seem, like, senseless, amount, borrow, normal, person, however, bubble, burst, ratio, even, high, mortgage, giant, fannie, may, freddie, mac, closely, link, government, supposedly, run, strict, standard, normal, leveraged, close, surf, little, internet, investment, agent, find, say, okay, leveraged, one, borrow, put, risky, investment, lesson, learnt, john, stepek, put, money, week, recently, way, stop, future, crisis, prevent, level, leverage, system, reach, point, become, dangerous, leverage, ratio, sensible, limit, need, ban, problem, easy, find, support, policy, like, crisis, economy, many, powerful, voice, will, insist, regulation, put, unreasonable, brake, economic, activity),(802,[1,2,4,7,9,11,13,15,17,18,19,20,21,22,23,24,25,28,29,33,34,35,36,39,41,45,46,47,48,50,51,53,54,56,59,60,61,62,69,73,75,76,78,83,92,93,94,96,97,98,99,100,101,102,105,106,108,109,112,114,117,121,123,125,127,128,130,132,133,138,141,145,151,156,164,167,168,169,173,175,181,182,187,188,190,193,194,199,202,204,205,209,212,214,218,222,224,227,236,239,241,248,254,255,260,261,268,271,276,278,280,284,293,294,296,297,306,307,309,313,317,319,324,326,328,333,335,350,362,364,365,372,373,379,386,387,390,393,395,409,412,420,427,434,445,446,452,453,456,457,460,465,466,473,475,496,499,519,526,531,540,546,549,550,556,557,563,567,568,570,581,584,585,587,594,599,602,603,604,605,610,611,613,629,630,641,662,664,666,683,684,695,698,699,701,702,709,718,721,733,735,736,741,742,743,744,745,747,750,751,753,755,758,763,770,771,772,774,780,781,783,785,789,795,798,799],[7.0,9.0,5.0,4.0,5.0,4.0,2.0,4.0,11.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,9.0,5.0,3.0,8.0,2.0,2.0,2.0,3.0,1.0,2.0,2.0,2.0,1.0,2.0,2.0,3.0,2.0,1.0,3.0,6.0,2.0,5.0,1.0,3.0,2.0,5.0,2.0,2.0,1.0,4.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,4.0,1.0,2.0,2.0,2.0,4.0,1.0,4.0,3.0,4.0,4.0,1.0,1.0,1.0,1.0,3.0,3.0,2.0,1.0,3.0,3.0,1.0,2.0,1.0,1.0,2.0,3.0,3.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]),(802,[1,2,4,7,9,11,13,15,17,18,19,20,21,22,23,24,25,28,29,33,34,35,36,39,41,45,46,47,48,50,51,53,54,56,59,60,61,62,69,73,75,76,78,83,92,93,94,96,97,98,99,100,101,102,105,106,108,109,112,114,117,121,123,125,127,128,130,132,133,138,141,145,151,156,164,167,168,169,173,175,181,182,187,188,190,193,194,199,202,204,205,209,212,214,218,222,224,227,236,239,241,248,254,255,260,261,268,271,276,278,280,284,293,294,296,297,306,307,309,313,317,319,324,326,328,333,335,350,362,364,365,372,373,379,386,387,390,393,395,409,412,420,427,434,445,446,452,453,456,457,460,465,466,473,475,496,499,519,526,531,540,546,549,550,556,557,563,567,568,570,581,584,585,587,594,599,602,603,604,605,610,611,613,629,630,641,662,664,666,683,684,695,698,699,701,702,709,718,721,733,735,736,741,742,743,744,745,747,750,751,753,755,758,763,770,771,772,774,780,781,783,785,789,795,798,799],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.6246189861593985,0.28768207245178085,0.0,0.0,0.0,0.0,0.0,0.0,6.238324625039508,0.0,0.0,5.545177444479562,0.5753641449035617,0.0,0.5753641449035617,0.0,0.0,0.0,0.0,0.0,0.28768207245178085,0.0,0.5753641449035617,0.8630462173553426,0.5753641449035617,0.28768207245178085,0.8630462173553426,4.1588830833596715,0.5753641449035617,3.4657359027997265,0.28768207245178085,0.0,0.0,3.4657359027997265,0.0,0.0,0.0,1.1507282898071234,0.28768207245178085,2.772588722239781,0.28768207245178085,0.0,0.28768207245178085,0.0,0.28768207245178085,0.5753641449035617,0.5753641449035617,2.772588722239781,0.0,0.0,0.5753641449035617,0.0,2.772588722239781,0.28768207245178085,2.772588722239781,0.8630462173553426,2.772588722239781,2.772588722239781,0.0,0.0,0.0,0.28768207245178085,2.0794415416798357,2.0794415416798357,0.5753641449035617,0.0,2.0794415416798357,2.0794415416798357,0.28768207245178085,0.5753641449035617,0.28768207245178085,0.0,0.5753641449035617,2.0794415416798357,2.0794415416798357,0.28768207245178085,2.0794415416798357,0.28768207245178085,0.5753641449035617,0.0,2.0794415416798357,0.0,0.28768207245178085,0.28768207245178085,0.28768207245178085,1.3862943611198906,0.28768207245178085,0.28768207245178085,1.3862943611198906,1.3862943611198906,1.3862943611198906,0.28768207245178085,0.28768207245178085,0.28768207245178085,1.3862943611198906,1.3862943611198906,1.3862943611198906,1.3862943611198906,0.28768207245178085,0.28768207245178085,0.28768207245178085,1.3862943611198906,0.28768207245178085,0.28768207245178085,0.28768207245178085,0.28768207245178085,0.28768207245178085,1.3862943611198906,0.28768207245178085,1.3862943611198906,0.28768207245178085,1.3862943611198906,0.28768207245178085,0.28768207245178085,1.3862943611198906,1.3862943611198906,0.28768207245178085,0.28768207245178085,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])]\n[1,WrappedArray(person, think, einstein, first, decade, twentieth, century, come, theory, relativity, albert, quietly, work, away, patent, office, switzerland, entirely, manage, come, completely, new, theory, space, time, actually, wasn, quite, like, history, science, dreadfully, tedious, subject, will, skip, albert, many, predecessor, get, straight, good, bit, theory, relativity, question, call, theory, relativity, time, length, longer, absolutes, get, digital, watch, wrist, metre, ruler, desk, seem, like, absolute, second, centrimetre, must, alpha, centauri, stay, balcony, start, career, astronaut, fly, round, galaxy, incredible, speed, pretty, close, speed, light, sec, later, whiz, past, balcony, somehow, compare, watch, ruler, metre, ruler, small, watch, go, slower, mine, actually, wouldn, possible, human, eye, can, spot, thing, move, kind, speed, spaceship, rocket, nasty, thing, balcony, metre, away, practically, possible, fun, space, travel, unbelievable, speed, nothing, seem, change, chance, compare, measurement, time, length, back, home, see, something, odd, happen, introductions, einstein, talk, twin, paradox, one, year, old, twin, stay, earth, fresh, astronaut, school, set, space, voyage, travel, speed, light, year, space, mission, accomplish, turn, round, head, back, earth, time, land, know, board, clock, year, pass, now, year, old, fortunately, study, relativity, prepare, shock, see, twin, sister, now, year, old, conclusion, space, travel, really, really, fast, also, time, travel, travel, future, without, get, much, old, everything, relative, exactly, actually, idea, time, length, relative, speed, propose, first, way, explain, observation, puzzle, everyone, person, nineteenth, century, devise, sensitive, piece, apparatus, measure, speed, light, earth, rotate, space, idea, behind, experiment, easy, grasp, think, spacecraft, tiny, particle, light, call, photon, accelerate, away, sun, wear, special, goggles, enable, see, individual, photon, approach, sec, expect, see, photon, move, ever, slowly, past, side, window, spacecraft, common, sense, say, put, foot, gas, bit, overtake, photon, leave, crawl, along, behind, spacecraft, exceed, speed, light, scientist, discover, everyone, surprise, move, faster, light, doesn, whiz, past, window, slowly, always, whizz, past, speed, word, photon, always, win, nothing, travel, fast, light, explain, bizarre, finding, scientist, even, einstein, suggest, follow, result, make, sense, faster, travel, relative, speed, light, short, unit, length, become, slow, measurement, time, become, outside, observer, look, superfast, starship, photon, might, move, past, side, window, really, slowly, speed, approach, sec, board, clock, slow, amount, measurement, length, compress, photon, see, inside, starship, will, seem, whizzing, past, speed, still, first, gear, proof, yes, although, spacecraft, still, way, slow, astronaut, notice, effect, relativity, research, behaviour, subatomic, particle, give, clear, support, theory, laboratory, deep, within, swiss, mountain, watch, happen, subatomic, particle, whiz, circular, tunnel, attaining, speed, close, light, weird, thing, happen, unstable, particle, stay, alive, lot, longer, normally, weird, thing, can, expained, theory, relativity, want, live, long, possible, can, relativity, help, time, tick, slowly, travel, really, fast, won, help, enjoy, live, longe, spaceship, nothing, seem, change, make, despite, health, risk, space, travel, osteoporosis, exposure, really, nasty, radiation, etc, will, still, look, old, wrinkled, however, able, come, back, earth, find, live, longer, old, mate, now, peace, cemetery, doesn, sound, like, fun, want, live, longer, good, sticking, healthy, diet, regular, exercise, couple, marriage, sincere, belief, god, average, marry, believer, live, longer, unmaried, atheist, just, one, theory, relativity, unfortunately, two, early, one, space, time, speed, light, know, special, theory, relativity, later, einstein, realise, make, important, omission, gravity, acceleration, turn, striking, similarity, develope, general, theory, relativity, add, complete, early, theory, einstein, wasn, first, say, pretty, weird, thing, light, gravity, space, go, bother, bore, historical, detail, let, concentrate, weird, stuff, weird, reality, accord, general, theory, relativity, well, one, space, curve, space, wasn, curve, whenever, shone, beam, light, like, laser, travel, line, seem, perfectly, straight, wherever, universe, ever, ever, direction, exactly, euclid, predict, euclid, ancient, greek, guy, found, father, high, school, geometry, assume, space, just, flat, happen, though, light, bent, gravity, beam, light, pass, galaxy, curve, come, close, strong, gravitational, field, person, even, think, gravity, bend, space, entire, universe, huge, sphere, practise, mean, tried, shine, laser, beam, beyond, edge, universe, gravity, bend, send, huge, circle, run, round, perimeter, universe, way, look, beyond, travel, beyond, edge, universe, like, weird, get, exactly, theory, predict, first, time, existence, black, hole, gravity, bend, light, possible, star, become, dense, enough, gravitational, field, great, light, previously, emit, longer, escape, let, begin, like, launch, spaceship, surface, earth, reach, velocity, hour, sec, otherwise, gravity, will, either, pull, orbit, back, surface, earth, escape, velocity, increase, relative, size, planet, star, even, galaxy, density, surface, sun, much, big, slightly, dense, escape, velocity, sec, cause, problem, terrestrial, spacecraft, cause, problem, light, travel, sec, star, reach, end, life, strange, thing, start, happen, start, collapse, eventually, atom, squeeze, together, tightly, nucleii, start, touch, one, another, make, collapse, star, incredibly, dense, consequence, incredibly, strong, gravitational, field, happen, sun, become, compact, diameter, mere, gravity, surface, high, light, dye, star, longer, able, escape, one, physicist, put, come, time, sun, shroud, darkness, light, emit, gravitational, field, will, impermeable, light, sun, become, black, hole, hang, collapse, star, can, become, black, hole, black, hole, can, really, hole, can, true, actually, weren, originally, call, black, hole, word, hole, bit, confuse, make, think, really, nothing, be, true, black, hole, something, either, big, dense, another, thing, didn, say, light, always, travel, sec, now, tell, gravity, make, light, travel, slowly, even, bring, standstill, somewhere, near, black, hole, measure, speed, light, come, board, laser, disappoint, find, still, travel, usual, speed, gravity, also, weird, thing, clock, ruler, use, measure, speed, light, close, strong, source, gravity, clock, tick, away, slowly, ruler, shrink, notice, inside, spaceship, distortion, time, space, call, warp, spacetime, black, hole, weird, thing, clock, help, live, longer, find, nearby, black, hole, spin, fly, spaceship, whirl, ring, material, around, quick, burst, booster, rocket, pull, ship, orbit, get, suck, blackness, board, atomic, clock, might, indicate, hair, raise, trip, just, last, couple, hour, back, mother, ship, hundred, year, might, elapse, old, mate, dead, isn, much, fun, however, happy, lesson, learnt, back, earth, bear, mind, clock, tick, slow, strong, gravitational, field, next, look, somewhere, new, live, physicist, put, atomic, clock, can, measure, billionth, second, basement, top, floor, skyscraper, prove, clock, basement, run, slowly, stop, look, room, view, get, mate, together, share, one, big, flat, underground),(802,[0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,26,27,28,29,30,32,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,59,61,63,65,66,67,68,70,71,73,75,77,78,80,82,83,84,85,86,87,88,89,90,91,92,93,94,95,97,98,99,100,103,105,107,108,109,110,111,112,113,114,115,116,118,119,120,121,122,124,129,130,131,132,133,134,136,137,138,142,143,146,149,150,151,154,156,157,159,160,161,162,165,166,168,170,171,172,173,174,175,177,179,181,183,184,185,186,188,189,192,193,195,196,197,198,199,200,201,203,204,205,206,207,208,211,212,213,215,218,219,220,221,222,226,230,231,232,233,234,235,237,238,240,245,246,247,249,251,256,257,259,262,26"
        },
        {
          "metadata" : { },
          "data" : {
            "text/html" : ""
          },
          "output_type" : "execute_result",
          "execution_count" : 51,
          "time" : "Took: 19.512s, at 2018-01-09 10:35"
        }
      ]
    }
  ],
  "nbformat" : 4
}