{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark._\n",
        "import org.apache.spark.SparkContext._\n",
        "import org.apache.spark.rdd._\n",
        "\n",
        "//import org.apache.spark.mllib.feature.{HashingTF, IDF}\n",
        "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
        "\n",
        "// import org.apache.spark.ml.feature.StopWordsRemover // TODO:\n",
        "\n",
        "import org.apache.spark.sql.{Row, SparkSession}\n",
        "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
        "\n",
        "//import org.apache.spark.mllib.feature.HashingTF._\n",
        "//import org.apache.spark.mllib.feature.IDF._\n",
        "\n",
        "//import org.apache.spark.mllib.linalg.Vectors\n",
        "//import org.apache.spark.mllib.linalg.Vector\n",
        "//import org.apache.spark.sql.Row\n",
        "\n",
        "//import org.apache.spark.sql.SQLContext // test\n",
        "\n",
        "import scala.collection.JavaConversions._\n",
        "import scala.util.control.Breaks._\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import opennlp.tools.langdetect._\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "//import opennlp.tools.util.Span\n",
        "\n",
        "object Helpers {\n",
        "    def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "        try {\n",
        "            f(resource)\n",
        "        } finally {\n",
        "            resource.close()\n",
        "        }\n",
        "}\n",
        "\n",
        "import Helpers._\n",
        "\n",
        "val de : String = \"de\"\n",
        "val en : String = \"en\"\n",
        "val fr : String = \"fr\"\n",
        "val langs : List[String] = List(de, en, fr)\n",
        "\n",
        "\n",
        "val conf = new SparkConf().setAppName(\"words\")\n",
        "conf.set(\"spark.driver.allowMultipleContexts\", \"true\");\n",
        "conf.setMaster(\"local\");\n",
        "val sparkContext = new SparkContext(conf)\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "\n",
        "val spark = SparkSession.builder().appName(\"words\").master(\"local\").getOrCreate()\n",
        "\n",
        "def detectLang(line : String, dicts : scala.collection.mutable.Map[String, List[String]]) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => dicts.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "}\n",
        "\n",
        "val stopwordsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty \n",
        "// Stopwords per each lang, are all stored in RAM, because used for all supported languages detection & they're relatively small, quick to read.  \n",
        "\n",
        "for(lang <- langs) { // main loop. TODO: refactor into function and call it async\n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-stopwords.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      val list = stopwordsPerLang.getOrElse(lang, List.empty)\n",
        "      stopwordsPerLang.update(lang, list:+line)  \n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val textfilesPathsPerLang: scala.collection.mutable.Map[String, List[String]] = scala.collection.mutable.Map.empty\n",
        "val textfiles = Files.newDirectoryStream(Paths.get(\"notebooks/words/text-data\"))\n",
        "                     .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "\n",
        "for(filePath <- textfiles) {\n",
        "  val file = filePath.toString\n",
        "  using(scala.io.Source.fromFile(file)) { source => \n",
        "    val firstLine = source.getLines.next() // get single first line & detect language with it, TODO: better to use a few random lines in the middle of the text\n",
        "    detectLang(firstLine, stopwordsPerLang) match  {\n",
        "      case Some(lang) => {\n",
        "        var list = textfilesPathsPerLang.getOrElse(lang, List.empty)\n",
        "        textfilesPathsPerLang.update(lang, list:+file)        \n",
        "      }\n",
        "      case None => println(\"Language was not detected for file: $file\")\n",
        "    }                                              \n",
        "  }\n",
        "}\n",
        "/*\n",
        "case class Lemma(entry:String, default:String, pos:String)\n",
        "object Lemma {\n",
        "  def parse(line:String): Option[Lemma] = {\n",
        "    line.split(\"\\t\") match { \n",
        "      case Array(f,s,t,_*) => Some(Lemma(f, s, t))\n",
        "      case _ => None\n",
        "    }\n",
        "  }  \n",
        "}\n",
        "*/\n",
        "// ?\n",
        "def removeTextNoise(text:String) : String = {\n",
        "  val removedNumbers = text.filter(!_.isDigit)\n",
        "  // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "  val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}&&[^.]]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \"\")\n",
        "  // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "  val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \"\")\n",
        "  val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "  replacedEscapeSeqWithSpace\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        "def readLemmas(lang:String) : scala.collection.mutable.ListBuffer[Lemma] = {\n",
        "  var lemmas : scala.collection.mutable.ListBuffer[Lemma] = scala.collection.mutable.ListBuffer.empty  \n",
        "  using(scala.io.Source.fromFile(s\"notebooks/words/vocabs/$lang-lemmatizer.txt\")) { source => \n",
        "    for (line <- source.getLines) {\n",
        "      Lemma.parse(line) match { \n",
        "        case Some(lemma) => lemmas+=lemma\n",
        "        case _ =>;\n",
        "      }      \n",
        "    }\n",
        "  }\n",
        "  lemmas\n",
        "}\n",
        "*/\n",
        "/*\n",
        "  def initTokenizer(lang:String) = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      val model : TokenizerModel = new TokenizerModel(stream)\n",
        "      new TokenizerME(model);\n",
        "    }\n",
        "*/\n",
        "class OpenNlpTokenizer(val model: TokenizerModel) {//extends Tokenizer {\n",
        "  def this(lang:String) = this(OpenNlpTokenizer.loadDefaultModel(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(model)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    //assume(positions.length == strings.length)\n",
        "    //for ((pos, string) <- (positions.iterator zip strings.iterator).toList)\n",
        "    //yield new Token(string, pos.getStart)\n",
        "    strings\n",
        "  }  \n",
        "}\n",
        "\n",
        "object OpenNlpTokenizer {\n",
        "  def loadDefaultModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"notebooks/words/vocabs/$lang-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "/*\n",
        "object OpenNlpTokenizerMain extends TokenizerMain {\n",
        "  val tokenizer = new OpenNlpTokenizer()\n",
        "}\n",
        "*/\n",
        "\n",
        "def removeStopWords(lang: String, tokens:Seq[String]) : Seq[String] = {\n",
        "   tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "}\n",
        "\n",
        "\n",
        "for ((lang,textsPaths) <- textfilesPathsPerLang) { // main loop. TODO: refactor into function and call it async\n",
        "   //val lemmas = readLemmas(lang)\n",
        "  val tokenizer = new OpenNlpTokenizer(lang)\n",
        "  \n",
        "  for (paths <- textsPaths) {\n",
        "    using(scala.io.Source.fromFile(paths)) { source => \n",
        "      val text = source.getLines.mkString // getLines.toList for RDD...\n",
        "      val unnoisedText = removeTextNoise(text)                                      \n",
        "      val tokens = tokenizer.tokenize(unnoisedText)\n",
        "      val tokensWithoutStopWords = removeStopWords(lang, tokens)\n",
        "      //val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\n",
        "      //import sqlContext.implicits._\n",
        "         // : RDD[Seq[String]]                                    \n",
        "      //val rdd = sparkContext.parallelize(tokens)\n",
        "                                            //.map{ x:Row => x.getAs[String](0)}\n",
        "                                            //.map(x => Tuple1(x.split(\",\"))) // wrapping\n",
        "      //val tf: RDD[Vector] = new HashingTF().transform(rdd)\n",
        "      //tf.cache()                                                                   \n",
        "      //val idf = new IDF().fit(tf)\n",
        "      //val tfidf: RDD[Vector] = idf.transform(tf)\n",
        "      //tfidf.foreach(x => println(x))\n",
        "      \n",
        "      //val tf = new HashingTF().transform(rdd)\n",
        "      //val idf = new IDF().fit(tf)\n",
        "      //val tfidf = idf.transform(tf)\n",
        "      /*for(item <- tf){\n",
        "        print(item)\n",
        "      }*/\n",
        "      //rescaledData.select(\"features\").show()\n",
        "      \n",
        "      //val spark: SparkSession\n",
        "      // Seq(1, tokens)\n",
        "      val df = spark.createDataFrame(Array((1, tokensWithoutStopWords))).toDF(\"id\", \"tokens\") \n",
        "\n",
        "      val model: CountVectorizerModel = new CountVectorizer()\n",
        "          .setInputCol(\"tokens\")\n",
        "          .setOutputCol(\"features\")\n",
        "          .fit(df)\n",
        "      model.vocabulary\n",
        "                                                      \n",
        "    }\n",
        "  }  \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 2, localhost, executor driver): java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:347)\n\tat scala.None$.get(Option.scala:345)\n\tat org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)\n\tat org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n  at org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:176)\n  at $anonfun$12$$anonfun$apply$7$$anonfun$apply$8.apply(<console>:253)\n  at $anonfun$12$$anonfun$apply$7$$anonfun$apply$8.apply(<console>:221)\n  at Helpers$.using(<console>:74)\n  at $anonfun$12$$anonfun$apply$7.apply(<console>:221)\n  at $anonfun$12$$anonfun$apply$7.apply(<console>:220)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at $anonfun$12.apply(<console>:220)\n  at $anonfun$12.apply(<console>:216)\n  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n  at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n  ... 39 elided\nCaused by: java.util.NoSuchElementException: None.get\n  at scala.None$.get(Option.scala:347)\n  at scala.None$.get(Option.scala:345)\n  at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)\n  at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n  at java.lang.Thread.run(Thread.java:748)\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}