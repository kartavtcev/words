{
  "metadata" : {
    "id" : "edf4618b-5e50-4d09-882b-37217cb9411a",
    "name" : "words",
    "user_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T03:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : [
      "org.apache.opennlp % opennlp-tools % 1.8.4"
    ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : false,
        "id" : "55564D0646A343D2AF9ED4CD0B6D4953"
      },
      "cell_type" : "code",
      "source" : [
        "import scala.collection.JavaConversions._\n",
        "import scala.concurrent._\n",
        "import ExecutionContext.Implicits.global\n",
        "import scala.language.postfixOps\n",
        "//import util.control.Breaks._\n",
        "\n",
        "import java.io._\n",
        "import java.nio.file.{Files, Path, Paths}\n",
        "\n",
        "import org.apache.spark._\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.sql.{Row, SparkSession, Dataset}\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
        "\n",
        "//import opennlp.tools.langdetect._ // custom implementation\n",
        "import opennlp.tools.lemmatizer.DictionaryLemmatizer\n",
        "import opennlp.tools.postag.{POSModel, POSTaggerME}\n",
        "import opennlp.tools.tokenize.{TokenizerME, TokenizerModel}\n",
        "\n",
        "def using[A <: { def close(): Unit }, B](resource: A)(f: A => B): B =\n",
        "  try {\n",
        "      f(resource)\n",
        "  } catch {\n",
        "      case _ : Throwable => throw new Exception(\"file exception\")\n",
        "  } finally {\n",
        "      resource.close()\n",
        "  }\n",
        "\n",
        "type MapLangToStrings = scala.collection.immutable.Map[String, Seq[String]]\n",
        "\n",
        "// since sparkContext is unavailable as var here, I can't just use it like in \"Spark Dataset 101\", \"Spark 101\"\n",
        "// could be related to customDepts bug, because I added opennlp dependency:\n",
        "// https://github.com/spark-notebook/spark-notebook/issues/563\n",
        "\n",
        "object Spark {\n",
        "  val session = SparkSession\n",
        "    .builder()\n",
        "    .appName(\"words\")\n",
        "    .config(\"spark.driver.allowMultipleContexts\", \"true\")\n",
        "    .master(\"local\")\n",
        "    .getOrCreate()\n",
        "}\n",
        "import Spark.session.implicits._\n",
        "\n",
        "object NLP { \n",
        "  val numOfTopWords = 30  \n",
        "  \n",
        "  val vocabsRelativePath = \"notebooks/words/vocabs/\"\n",
        "  val textsRelativePath = \"notebooks/words/text-data/\"\n",
        "  val outputPath = \"notebooks/words/output/\"\n",
        "  val langNotDetected = \"lang-not-detected\"\n",
        "  \n",
        "  def getLangs : Seq[String] = {\n",
        "    val de : String = \"de\"\n",
        "    val en : String = \"en\"\n",
        "    val fr : String = \"fr\"\n",
        "    Seq(de, en, fr)\n",
        "  }\n",
        "  \n",
        "  def getStopwordsPerLang(langs : Seq[String]) : MapLangToStrings = {\n",
        "    langs map { lang => (lang, using(scala.io.Source.fromFile(s\"${vocabsRelativePath}${lang}-stopwords.txt\")) { \n",
        "                                 source => source.getLines.toList })\n",
        "    } toMap  \n",
        "  }\n",
        "  \n",
        "  def getFilesPaths : Seq[String] = {\n",
        "      Files.newDirectoryStream(Paths.get(textsRelativePath))\n",
        "           .filter(_.getFileName.toString.endsWith(\".txt\"))\n",
        "           .map(_.toString)\n",
        "           .toSeq\n",
        "  }\n",
        "  \n",
        "  def writeProcessedFile(ds: Dataset[ProcessedFile]) = {}\n",
        "  def writeTfIdfJson(ds: Dataset[TfIdfFile]) = { ds.na.fill(\"\")\n",
        "                                                   .coalesce(1)\n",
        "                                                   .write\n",
        "                                                   .format(\"json\")\n",
        "                                                   .mode(\"append\")\n",
        "                                                   .json(s\"${outputPath}/tfidf_files\") \n",
        "                                               }\n",
        "}\n",
        "\n",
        "case class Keyword(val keyword: String, val value: Double) extends Serializable\n",
        "case class TfIdfFile(val filename: String, val language:String, val features : Seq[Keyword]) extends Serializable\n",
        "case class ProcessedFile(val filename: String, val words: Seq[String]) extends Serializable\n",
        "\n",
        "class NLP(val stopwordsPerLang: MapLangToStrings, val textfilesPaths: Seq[String]) extends Serializable {\n",
        "  def this() = this(NLP.getStopwordsPerLang(NLP.getLangs), NLP.getFilesPaths)\n",
        "\n",
        "  def process(spark: SparkSession) : Seq[Either[(String, Seq[String]),(Dataset[ProcessedFile], Dataset[TfIdfFile])]] = {\n",
        "    getFilePathsPerLang(textfilesPaths) map { case (lang, textPaths) => //Future {\n",
        "        if(lang == NLP.langNotDetected) Left((lang, textPaths))\n",
        "                                             \n",
        "        val onlp = new OpenNLP(lang)\n",
        "  \n",
        "        val ls : Seq[(String,String,Array[String])] = textPaths map { path =>\n",
        "          using(scala.io.Source.fromFile(path)) { source =>\n",
        "            val text = source.getLines.mkString\n",
        "            val unnoisedText = removeTextNoise(text)\n",
        "                                               \n",
        "            val tokens = onlp.tokenize(unnoisedText)\n",
        "            val tokensExcludeStopWords = removeStopWords(lang, tokens, stopwordsPerLang)\n",
        "\n",
        "            val lemmas = onlp.lemmatize(tokensExcludeStopWords)\n",
        "            val lemmd = (tokensExcludeStopWords zip lemmas) map (tuple => if(tuple._2 != \"O\") tuple._2 else tuple._1 ) // if no lemma => original\n",
        "            (lang,path.split(\"/\").takeRight(1).head,lemmd.toArray)\n",
        "          }}\n",
        "        val df = spark.createDataFrame(ls).toDF(\"filename\",\"language\", \"tokens\") \n",
        "        val tf = new CountVectorizer()\n",
        "              .setInputCol(\"tokens\")\n",
        "              .setOutputCol(\"tf\")        \n",
        "\n",
        "        val idf = new IDF()\n",
        "              .setInputCol(\"tf\")\n",
        "              .setOutputCol(\"tfidf\")\n",
        "        \n",
        "        val va = new VectorAssembler()\n",
        "              .setInputCols(Array(\"tfidf\"))\n",
        "              .setOutputCol(\"features\")\n",
        "                                             \n",
        "        val tfidfPipeline = new Pipeline().setStages(Array(tf, idf, va))\n",
        "        val pipelineModel = tfidfPipeline.fit(df)\n",
        "        val tfidf =  pipelineModel.transform(df)\n",
        "        \n",
        "        val cvModel = pipelineModel.stages(0).asInstanceOf[CountVectorizerModel]      \n",
        "        val vocabulary = cvModel.vocabulary\n",
        "                                             \n",
        "        val tfIdfFilesJson = tfidf.select(\"filename\", \"language\", \"features\")\n",
        "                                  .map { case Row(filename: String, language: String, features: Vector) \n",
        "                                           => TfIdfFile(filename, language, features.toArray.toSeq\n",
        "                                                                                    .zipWithIndex\n",
        "                                                                                    .sortBy(- _._1)\n",
        "                                                                                    .take(NLP.numOfTopWords)\n",
        "                                                                                    .map(f => Keyword(vocabulary(f._2), f._1)))\n",
        "                                  } //.collect() - no cluster, local\n",
        "        val processedFiles = df.select(\"filename\", \"tokens\")\n",
        "                               .map { case Row(filename: String, tokens: Array[String])\n",
        "                                          => ProcessedFile(filename, tokens.toSeq)\n",
        "                               } //.collect() - no cluster, local\n",
        "        Right((processedFiles, tfIdfFilesJson))\n",
        "    } toSeq\n",
        "  }\n",
        "  \n",
        "  //def printType[T](x:T) : Unit =  { println(x.getClass.toString()) }\n",
        "  \n",
        "  def removeTextNoise(text:String) : String = {\n",
        "    val removedNumbers = text.filter(!_.isDigit)\n",
        "    // https://stackoverflow.com/questions/30074109/removing-punctuation-marks-form-text-in-scala-spark\n",
        "    val removedWordsOfSizeLessEqual2AndPunctuation = removedNumbers.replaceAll(\"\"\"([\\p{Punct}]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \" \")\n",
        "    // https://stackoverflow.com/questions/6198986/how-can-i-replace-non-printable-unicode-characters-in-java\n",
        "    val removedUnicodes = removedWordsOfSizeLessEqual2AndPunctuation.replaceAll(\"\"\"[\\p{C}]\"\"\", \" \")\n",
        "    val replacedEscapeSeqWithSpace =  removedUnicodes.replaceAll(\"\"\"[\\t\\n\\r\\f\\v]\"\"\", \" \")\n",
        "    replacedEscapeSeqWithSpace\n",
        "  }\n",
        "\n",
        "  def removeStopWords(lang: String, tokens:Seq[String], stopwordsPerLang : MapLangToStrings) : Seq[String] = {\n",
        "     tokens.filter(!stopwordsPerLang(lang).contains(_))\n",
        "  }\n",
        "  \n",
        "  def getFilePathsPerLang(textfilePaths : Seq[String]) : MapLangToStrings = {\n",
        "    textfilePaths map { file => \n",
        "      using(scala.io.Source.fromFile(file)) { source => \n",
        "        val firstLine = source.getLines.next() // detect language with first line, TODO: use a few random lines in the middle of the text\n",
        "        detectLang(firstLine, stopwordsPerLang) match  {\n",
        "          case Some(lang) => (lang, file)    \n",
        "          case None => (NLP.langNotDetected, file)\n",
        "        }                                              \n",
        "      }    \n",
        "    } groupBy(_._1) map { case (lang, group) => (lang, group.map(_._2)) }\n",
        "  } \n",
        "  \n",
        "  /*\n",
        "    Before I googled Apache OpenNLP, I implemented custom language recognizer based on -stopwords.txt.\n",
        "    Since some external libs are using dictionary approach anyway (https://github.com/optimaize/language-detector):\n",
        "    stopwords are commonly found in the speech,\n",
        "    stopwords dictionary is relatively small and stopwords of 3 langs provided differ a lot.\n",
        "  */\n",
        "  def detectLang(line : String, stopwordsPerLang : MapLangToStrings) : Option[String] = {\n",
        "    val langs = line.split(\" \").flatMap(item => stopwordsPerLang.filter(_._2.exists(_.equalsIgnoreCase(item))).map(_._1))\n",
        "                    .groupBy(f => f)\n",
        "                    .map(g => (g._1, g._2.size))\n",
        "    if(langs.isEmpty) None\n",
        "    else Some(langs.maxBy(_._2)._1)\n",
        "  } \n",
        "}\n",
        "\n",
        "class OpenNLP(val tokenizerModel: TokenizerModel, val posModel : POSModel, val lemmatizer : DictionaryLemmatizer) extends Serializable {\n",
        "  def this(lang:String) = this(OpenNLP.loadTokenizerModel(lang), OpenNLP.loadPOSModel(lang), OpenNLP.loadLemmatizer(lang))\n",
        "\n",
        "  val tokenizer = new TokenizerME(tokenizerModel)\n",
        "  val posTagger = new POSTaggerME(posModel)\n",
        "\n",
        "  def tokenize(text: String): Seq[String] = {\n",
        "    val positions = tokenizer.tokenizePos(text)\n",
        "    val strings = positions.map {\n",
        "      pos => text.substring(pos.getStart, pos.getEnd)\n",
        "    }\n",
        "    strings.filter(_.length > 1).map(s => s.toLowerCase) // additional cleanup after regexps & to lower case\n",
        "  }\n",
        "  \n",
        "  def lemmatize(tokens:Seq[String]): Seq[String] = {\n",
        "    val tags = posTagger.tag(tokens.toArray)\n",
        "    lemmatizer.lemmatize(tokens.toArray, tags)\n",
        "  }\n",
        "} \n",
        "\n",
        "object OpenNLP {\n",
        "  def loadTokenizerModel(lang:String): TokenizerModel = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-token.bin\")) { stream =>\n",
        "      new TokenizerModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadPOSModel(lang:String): POSModel = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-pos-maxent.bin\")) { stream =>\n",
        "      new POSModel(stream)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  def loadLemmatizer(lang:String): DictionaryLemmatizer = {\n",
        "    using(new FileInputStream(s\"${NLP.vocabsRelativePath}${lang}-lemmatizer-columns-reordered.txt\")) { stream =>\n",
        "      new DictionaryLemmatizer(stream)\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "val nlp = new NLP()\n",
        "nlp process(Spark.session) foreach ( r => r match {\n",
        "                                                case Right((file, tfidfjson)) => \n",
        "                                                    { NLP writeProcessedFile(file); NLP writeTfIdfJson(tfidfjson); } \n",
        "                                                case Left((lang, files)) => \n",
        "                                                    files foreach (f => println(\"Lang: $lang is not detected for file: $f\"))\n",
        "                                                  })"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "output_type" : "stream",
          "text" : "org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:473)\n  at NLP$.writeTfIdfJson(<console>:119)\n  at $anonfun$10.apply(<console>:272)\n  at $anonfun$10.apply(<console>:270)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  ... 37 elided\nCaused by: org.apache.spark.SparkException: Task not serializable\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:298)\n  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)\n  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)\n  at org.apache.spark.SparkContext.clean(SparkContext.scala:2094)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:840)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:839)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:839)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:371)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.CoalesceExec.doExecute(basicPhysicalOperators.scala:509)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)\n  ... 61 more\nCaused by: java.io.NotSerializableException: NLP$\nSerialization stack:\n\t- object not serializable (class: NLP$, value: NLP$@1997978f)\n\t- field (class: $iw, name: NLP$module, type: class NLP$)\n\t- object (class $iw, $iw@4a18dd97)\n\t- field (class: $iw, name: $iw, type: class $iw)\n\t- object (class $iw, $iw@1feb8b8)\n\t- field (class: $line117.$read, name: $iw, type: class $iw)\n\t- object (class $line117.$read, $line117.$read@3c49ff0e)\n\t- field (class: $iw, name: $line117$read, type: class $line117.$read)\n\t- object (class $iw, $iw@7647395e)\n\t- field (class: $iw, name: $outer, type: class $iw)\n\t- object (class $iw, $iw@6262e440)\n\t- field (class: NLP, name: $outer, type: class $iw)\n\t- object (class NLP, NLP@5c64a0ef)\n\t- field (class: NLP$$anonfun$process$1, name: $outer, type: class NLP)\n\t- object (class NLP$$anonfun$process$1, <function1>)\n\t- field (class: NLP$$anonfun$process$1$$anonfun$3, name: $outer, type: class NLP$$anonfun$process$1)\n\t- object (class NLP$$anonfun$process$1$$anonfun$3, <function1>)\n\t- field (class: org.apache.spark.sql.catalyst.expressions.Literal, name: value, type: class java.lang.Object)\n\t- object (class org.apache.spark.sql.catalyst.expressions.Literal, <function1>)\n\t- element of array (index: 1)\n\t- array (class [Ljava.lang.Object;, size 7)\n\t- field (class: org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8, name: references$1, type: class [Ljava.lang.Object;)\n\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8, <function2>)\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)\n  ... 87 more\n"
        }
      ]
    }
  ],
  "nbformat" : 4
}